# Smart Fasal - Theory and ML Models Documentation

## Table of Contents
1. [System Overview](#system-overview)
2. [Machine Learning Models](#machine-learning-models)
3. [Model 1: Crop Disease Detection](#model-1-crop-disease-detection)
4. [Model 2: Crop Recommendation](#model-2-crop-recommendation)
5. [Technical Architecture](#technical-architecture)
6. [Data Flow](#data-flow)
7. [Implementation Details](#implementation-details)

---

## System Overview

**Smart Fasal** is an AI-powered precision agriculture platform designed to help farmers make data-driven decisions about crop management. The platform leverages multiple machine learning models to provide:

- **Crop Disease Detection**: Identifies plant diseases from leaf images
- **Crop Health Monitoring**: Tracks crop health metrics over time
- **Yield Prediction**: Forecasts harvest yields based on various factors
- **Field Efficiency Optimization**: Recommends optimal farming practices
- **Harvest Planning**: Schedules optimal harvest timing
- **Crop Recommendation**: Suggests best crops based on soil and weather

### Key Technologies
- **Frontend**: React.js, TypeScript, Tailwind CSS
- **Backend**: FastAPI (Python), Supabase
- **ML Framework**: TensorFlow/Keras
- **Database**: PostgreSQL (via Supabase)
- **Storage**: Supabase Storage
- **Authentication**: Supabase Auth

---

## Machine Learning Models

### Overview of ML Models in Smart Fasal

The platform integrates multiple ML models, each serving a specific agricultural purpose:

| Model | Purpose | Input | Output | Status |
|-------|---------|-------|--------|--------|
| Crop Disease Detection | Identify plant diseases | Leaf images | Disease name, confidence, treatment | âœ… Implemented |
| Crop Recommendation | Suggest optimal crops | Soil + Weather data | Crop recommendations | ðŸ“Š Model Available |
| Yield Prediction | Forecast harvest yields | Historical + Current data | Expected yield | ðŸ”„ Planned |
| Field Efficiency | Optimize resource usage | Field parameters | Optimization suggestions | ðŸ”„ Planned |
| Harvest Timing | Schedule optimal harvest | Crop + Weather data | Optimal harvest date | ðŸ”„ Planned |

---

## Model 1: Crop Disease Detection

### Theory and Background

#### Problem Statement
Plant diseases cause significant crop losses globally, reducing agricultural productivity by 20-40%. Early detection is crucial for:
- Timely treatment application
- Preventing disease spread
- Minimizing economic losses
- Reducing pesticide usage

#### Solution Approach
Using **Deep Learning** and **Computer Vision** to automatically detect and classify plant diseases from leaf images.

### Dataset: PlantVillage

**Source**: Hughes & Salathe (2016) - "An Open Access Repository of Images on Plant Health"

**Statistics**:
- **Total Images**: ~87,000 leaf images
- **Training Set**: 70,295 images
- **Validation Set**: 17,572 images
- **Classes**: 38 disease categories
- **Plants Covered**: 14 crop species

**Crops Included**:
1. Apple
2. Blueberry
3. Cherry
4. Corn (Maize)
5. Grape
6. Orange
7. Peach
8. Pepper (Bell)
9. Potato
10. Raspberry
11. Soybean
12. Squash
13. Strawberry
14. Tomato

### Disease Classes (38 Total)

#### Apple (4 classes)
- Apple Scab
- Black Rot
- Cedar Apple Rust
- Healthy

#### Tomato (10 classes)
- Bacterial Spot
- Early Blight
- Late Blight
- Leaf Mold
- Septoria Leaf Spot
- Spider Mites (Two-spotted spider mite)
- Target Spot
- Yellow Leaf Curl Virus
- Mosaic Virus
- Healthy

#### Potato (3 classes)
- Early Blight
- Late Blight
- Healthy

#### Corn/Maize (4 classes)
- Cercospora Leaf Spot (Gray Leaf Spot)
- Common Rust
- Northern Leaf Blight
- Healthy

#### Grape (4 classes)
- Black Rot
- Esca (Black Measles)
- Leaf Blight (Isariopsis Leaf Spot)
- Healthy

#### And more... (38 total classes)

### Model Architecture: MobileNet

#### Why MobileNet?

**MobileNet** is a lightweight CNN architecture designed for mobile and embedded vision applications.

**Key Features**:
- **Efficient**: Uses depthwise separable convolutions
- **Fast**: ~100-200ms inference time per image
- **Compact**: Small model size (~16MB)
- **Accurate**: ~90% validation accuracy on PlantVillage
- **Mobile-friendly**: Can run on smartphones and edge devices

#### Architecture Details

```
Input Layer
    â†“
MobileNet Base (Pre-trained on ImageNet)
    - Depthwise Separable Convolutions
    - 28 layers
    - Width Multiplier: 1.0
    - Input: 224Ã—224Ã—3
    â†“
Global Average Pooling 2D
    - Reduces spatial dimensions
    - Output: 1024 features
    â†“
Dropout (0.2)
    - Prevents overfitting
    - Randomly drops 20% of neurons
    â†“
Dense Layer (38 units, Softmax)
    - Output: Probability for each of 38 classes
    â†“
Prediction
```

### Transfer Learning Approach

**Transfer Learning** is used to leverage pre-trained knowledge from ImageNet:

1. **Base Model**: MobileNet pre-trained on ImageNet (1.4M images, 1000 classes)
2. **Frozen Layers**: Base MobileNet layers are frozen (not trainable)
3. **Custom Head**: Only the top classification layers are trained
4. **Benefits**:
   - Faster training (fewer parameters to update)
   - Better generalization (learned features from ImageNet)
   - Requires less data (leverages existing knowledge)

### Training Process

#### Data Preprocessing

**Image Size**: 224Ã—224 pixels (MobileNet standard)

**Normalization**: Pixel values rescaled from [0, 255] to [0, 1]
```python
rescale = 1/255.0
```

**Data Augmentation** (Training only):
```python
- Rotation: Random rotations
- Width/Height Shift: Â±20% translation
- Shear: 20% shearing transformation
- Zoom: Â±20% zooming
- Fill Mode: "nearest" for new pixels
```

**Purpose of Augmentation**:
- Increases effective dataset size
- Prevents overfitting
- Improves model generalization
- Simulates real-world variations (lighting, angles, etc.)

#### Training Hyperparameters

```python
Optimizer: Adam
    - Learning Rate: 0.001 (default)
    - Beta1: 0.9
    - Beta2: 0.999
    - Epsilon: 1e-07

Loss Function: Categorical Crossentropy
Batch Size: 32
Epochs: 25
Steps per Epoch: 150
Validation Steps: 100
```

#### Training Results

| Epoch | Training Loss | Training Accuracy | Validation Loss | Validation Accuracy |
|-------|---------------|-------------------|-----------------|---------------------|
| 1     | 2.0143        | 47.33%            | 0.8028          | 80.59%              |
| 5     | 0.3931        | 88.40%            | 0.2898          | 91.00%              |
| 10    | ~0.25         | ~92%              | ~0.20           | ~94%                |
| 25    | ~0.15         | ~95%              | ~0.18           | ~95%                |

**Final Performance**:
- **Training Accuracy**: ~95%
- **Validation Accuracy**: ~90-95%
- **Inference Time**: 100-200ms per image

### Model Deployment

#### TFLite Conversion

The trained Keras model is converted to **TensorFlow Lite** format for efficient deployment:

```python
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

**Benefits of TFLite**:
- **Smaller Size**: Reduced from ~50MB to ~16MB
- **Faster Inference**: Optimized for mobile/edge devices
- **Lower Memory**: Uses less RAM during inference
- **Cross-Platform**: Runs on Android, iOS, embedded systems

#### Model Files

1. **SavedModel Format** (`plant_disease/`)
   - Full TensorFlow model
   - Size: ~50MB
   - Used for further training/fine-tuning

2. **TFLite Format** (`model.tflite`)
   - Optimized for inference
   - Size: ~16MB
   - Used in production backend

3. **Class Indices** (`class_indices.json`)
   - Maps class indices to disease names
   - 38 disease classes

### Prediction Pipeline

#### Step-by-Step Process

1. **Image Upload**
   - User uploads leaf image from frontend
   - Supported formats: JPEG, PNG, GIF, BMP
   - Size limit: 10MB

2. **Image Encoding**
   - Frontend converts image to Base64
   - Sent as JSON payload to backend

3. **Backend Processing**
   ```python
   a. Decode Base64 â†’ Binary image
   b. Load image using PIL
   c. Resize to 224Ã—224
   d. Normalize pixels (Ã· 255)
   e. Convert to numpy array
   f. Add batch dimension
   g. Convert to float32
   ```

4. **ML Inference**
   ```python
   # Using TFLite model
   interpreter.set_tensor(input_details[0]['index'], processed_image)
   interpreter.invoke()
   predictions = interpreter.get_tensor(output_details[0]['index'])
   ```

5. **Post-Processing**
   ```python
   # Get top prediction
   top_idx = np.argmax(predictions)
   confidence = predictions[0][top_idx]
   disease_name = class_indices[top_idx]
   
   # Determine severity
   if 'healthy' in disease_name.lower():
       severity = 'low'
   elif confidence > 0.8:
       severity = 'high'
   else:
       severity = 'medium'
   ```

6. **Generate Recommendations**
   - Based on disease type
   - Includes treatment methods
   - Provides prevention tips

7. **Save to Database**
   - Upload image to Supabase Storage
   - Save analysis results to PostgreSQL
   - Update user statistics

8. **Return Results**
   - Send JSON response to frontend
   - Display diagnosis, confidence, recommendations

### Output Format

```json
{
  "success": true,
  "analysis": {
    "predictions": [
      {
        "crop": "Tomato",
        "disease": "Late_blight",
        "confidence": 0.967,
        "class_name": "Tomato___Late_blight"
      }
    ],
    "top_prediction": {
      "crop": "Tomato",
      "disease": "Late_blight",
      "confidence": 0.967
    },
    "severity": "high",
    "recommendations": [
      "Remove and destroy infected plants immediately",
      "Apply fungicide (copper-based or chlorothalonil)",
      "Improve air circulation between plants",
      "Avoid overhead watering",
      "Rotate crops next season"
    ],
    "model_info": {
      "version": "1.0",
      "accuracy": "90%",
      "classes": 38
    }
  },
  "timestamp": "2025-10-30T12:34:56Z"
}
```

---

## Model 2: Crop Recommendation

### Theory and Background

#### Problem Statement
Farmers often struggle to decide which crop to plant based on:
- Soil nutrient composition (N, P, K)
- Weather conditions (temperature, humidity, rainfall)
- pH levels
- Regional suitability

**Wrong crop selection** leads to:
- Lower yields
- Wasted resources
- Economic losses
- Soil degradation

#### Solution Approach
Use **Machine Learning** to recommend optimal crops based on soil and weather parameters.

### Dataset: Crop Recommendation Dataset

**Features** (7 input variables):
1. **N** (Nitrogen content ratio in soil)
2. **P** (Phosphorous content ratio in soil)
3. **K** (Potassium content ratio in soil)
4. **Temperature** (Â°C)
5. **Humidity** (%)
6. **pH** (Soil pH value)
7. **Rainfall** (mm)

**Target Variable**: Crop label (22 classes)

**Dataset Size**: ~2,200 samples

**Crops** (22 classes):
1. Rice
2. Maize
3. Chickpea
4. Kidney Beans
5. Pigeon Peas
6. Moth Beans
7. Mung Bean
8. Black Gram
9. Lentil
10. Pomegranate
11. Banana
12. Mango
13. Grapes
14. Watermelon
15. Muskmelon
16. Apple
17. Orange
18. Papaya
19. Coconut
20. Cotton
21. Jute
22. Coffee

### Data Analysis

#### Feature Distributions

**Temperature & pH**:
- Both follow **normal distribution** (bell-shaped curve)
- Symmetrical around mean
- Shows that most readings are near average
- Occasional extreme values (outliers)

**Class Balance**:
- Dataset is **well-balanced**
- Each crop has similar number of samples (~100 each)
- No need for downsampling/upsampling

### Model Architecture Options

Several ML algorithms can be used:

1. **Random Forest Classifier**
   - Ensemble of decision trees
   - High accuracy (~95-98%)
   - Handles non-linear relationships
   - Feature importance ranking

2. **Support Vector Machine (SVM)**
   - Works well with numerical features
   - Good for multi-class classification

3. **Logistic Regression**
   - Simple, interpretable
   - Fast training and prediction

4. **Neural Network**
   - Can capture complex patterns
   - Requires more data

**Recommended**: Random Forest Classifier (best accuracy and interpretability)

### Training Process

#### Data Preprocessing

1. **Check for Missing Values**
   ```python
   # Heatmap visualization shows NO missing values
   sns.heatmap(df.isnull(), cmap="coolwarm")
   ```

2. **Feature Scaling**
   ```python
   # StandardScaler or MinMaxScaler
   # Normalize features to same scale
   from sklearn.preprocessing import StandardScaler
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X)
   ```

3. **Train-Test Split**
   ```python
   from sklearn.model_selection import train_test_split
   X_train, X_test, y_train, y_test = train_test_split(
       X, y, test_size=0.2, random_state=42
   )
   ```

#### Model Training

```python
from sklearn.ensemble import RandomForestClassifier

# Initialize model
model = RandomForestClassifier(
    n_estimators=100,    # 100 decision trees
    max_depth=20,        # Maximum tree depth
    random_state=42
)

# Train model
model.fit(X_train, y_train)

# Evaluate
accuracy = model.score(X_test, y_test)
print(f"Accuracy: {accuracy * 100:.2f}%")
```

**Expected Accuracy**: 95-99%

### Prediction Pipeline

```python
# Input: Soil and weather parameters
input_data = {
    'N': 90,           # Nitrogen
    'P': 42,           # Phosphorous
    'K': 43,           # Potassium
    'temperature': 20.88,
    'humidity': 82.00,
    'ph': 6.50,
    'rainfall': 202.94
}

# Preprocess
input_array = np.array([list(input_data.values())])
input_scaled = scaler.transform(input_array)

# Predict
prediction = model.predict(input_scaled)
probabilities = model.predict_proba(input_scaled)

# Output
crop_name = prediction[0]
confidence = max(probabilities[0]) * 100
```

### Integration Status

**Current Status**: Model code available, not yet integrated into backend

**Implementation Plan**:
1. Train and save model (`crop_recommendation.pkl`)
2. Create FastAPI endpoint `/recommend-crop`
3. Accept soil and weather parameters
4. Return recommended crops with confidence scores
5. Display on Dashboard or dedicated page

---

## Technical Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Frontend Layer                          â”‚
â”‚  React + TypeScript + Tailwind CSS + shadcn/ui              â”‚
â”‚  Port: 8080                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ HTTP/REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Backend Layer                           â”‚
â”‚  FastAPI + Python + Uvicorn                                 â”‚
â”‚  Port: 8000                                                  â”‚
â”‚                                                              â”‚
â”‚  Endpoints:                                                  â”‚
â”‚  - POST /analyze-base64    (Crop Disease Detection)        â”‚
â”‚  - GET  /health            (Health Check)                   â”‚
â”‚  - GET  /statistics        (User Analytics)                 â”‚
â”‚  - GET  /model/info        (Model Information)              â”‚
â”‚  - GET  /classes           (Available Classes)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML Services Layer                         â”‚
â”‚  CropDiseaseService                                         â”‚
â”‚  - Model: TFLite (MobileNet)                                â”‚
â”‚  - Input: 224Ã—224Ã—3 images                                  â”‚
â”‚  - Output: 38 disease classes                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Persistence Layer                     â”‚
â”‚  Supabase (PostgreSQL + Storage + Auth)                    â”‚
â”‚                                                              â”‚
â”‚  Tables:                                                     â”‚
â”‚  - crop_health_analysis  (Analysis results)                â”‚
â”‚  - fields               (Field information)                 â”‚
â”‚  - profiles             (User profiles)                     â”‚
â”‚                                                              â”‚
â”‚  Storage Buckets:                                           â”‚
â”‚  - crop-images          (Uploaded images)                   â”‚
â”‚  - field-data           (Field documents)                   â”‚
â”‚  - reports              (Generated reports)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Frontend**:
- React 18 (UI library)
- TypeScript (Type safety)
- Vite (Build tool)
- Tailwind CSS (Styling)
- shadcn/ui (Component library)
- React Router (Routing)
- React Query (Data fetching)

**Backend**:
- FastAPI (Web framework)
- Uvicorn (ASGI server)
- Pydantic (Data validation)
- Python 3.12

**ML Framework**:
- TensorFlow 2.19.0
- Keras (High-level API)
- TensorFlow Lite (Inference)
- NumPy (Numerical computing)
- Pillow (Image processing)

**Database & Services**:
- Supabase (BaaS)
- PostgreSQL (Relational database)
- Supabase Storage (Object storage)
- Supabase Auth (Authentication)

---

## Data Flow

### Complete Request-Response Cycle

```
1. User Action
   â””â”€> User uploads crop image on Crop Health page

2. Frontend Processing
   â”œâ”€> Validate file (size, type)
   â”œâ”€> Create image preview
   â”œâ”€> Convert image to Base64
   â””â”€> Send POST request to backend

3. Backend API (FastAPI)
   â”œâ”€> Receive JSON with Base64 image
   â”œâ”€> Validate request using Pydantic models
   â”œâ”€> Decode Base64 to binary
   â”œâ”€> Validate image format (PIL)
   â””â”€> Pass to ML service

4. ML Service (CropDiseaseService)
   â”œâ”€> Load image with PIL
   â”œâ”€> Resize to 224Ã—224
   â”œâ”€> Normalize pixels (Ã·255)
   â”œâ”€> Convert to numpy array (float32)
   â”œâ”€> Run TFLite inference
   â”œâ”€> Get predictions (38 probabilities)
   â”œâ”€> Extract top prediction
   â”œâ”€> Calculate severity
   â””â”€> Generate recommendations

5. Data Persistence (Supabase)
   â”œâ”€> Upload original image to Storage
   â”œâ”€> Get public URL
   â”œâ”€> Insert analysis record to database
   â”‚   â”œâ”€> user_id
   â”‚   â”œâ”€> field_id
   â”‚   â”œâ”€> image_url
   â”‚   â”œâ”€> diagnosis
   â”‚   â”œâ”€> confidence
   â”‚   â”œâ”€> severity
   â”‚   â””â”€> recommendations
   â””â”€> Return saved record

6. Backend Response
   â””â”€> Send JSON with analysis results

7. Frontend Updates
   â”œâ”€> Display analysis results
   â”œâ”€> Update statistics
   â”‚   â”œâ”€> Total Analyses
   â”‚   â”œâ”€> Health Score
   â”‚   â”œâ”€> Healthy Crops
   â”‚   â””â”€> Diseased Crops
   â”œâ”€> Show in Recent Analyses list
   â””â”€> Clear image preview
```

---

## Implementation Details

### Backend File Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app, routes
â”‚   â””â”€â”€ models.py            # Pydantic models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ crop_disease_service.py  # ML inference service
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.tflite         # TFLite model
â”‚   â”œâ”€â”€ plant_disease/       # SavedModel format
â”‚   â””â”€â”€ class_indices.json   # Class mappings
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ start.py                 # Server startup script
â””â”€â”€ .env                     # Environment variables
```

### Key Code Components

#### 1. Pydantic Models (`app/models.py`)

```python
class CropAnalysisRequest(BaseModel):
    image_data: str  # Base64 encoded
    field_id: Optional[str] = None
    crop_type: Optional[str] = None
    user_id: str

class CropAnalysisResponse(BaseModel):
    success: bool
    analysis: AnalysisResult
    timestamp: datetime
```

#### 2. FastAPI Endpoint (`app/main.py`)

```python
@app.post("/analyze-base64", response_model=CropAnalysisResponse)
async def analyze_crop_health_base64(request: CropAnalysisRequest):
    # Decode image
    image_bytes = base64.b64decode(request.image_data)
    
    # Analyze with ML model
    analysis_result = crop_disease_service.predict(image_bytes)
    
    # Return results
    return {
        "success": True,
        "analysis": analysis_result,
        "timestamp": datetime.now()
    }
```

#### 3. ML Service (`services/crop_disease_service.py`)

```python
class CropDiseaseService:
    def __init__(self):
        self.model = self._load_tflite_model()
        self.class_indices = self._load_class_indices()
    
    def predict(self, image_bytes):
        # Preprocess
        processed_image = self._preprocess_image(image_bytes)
        
        # Inference
        predictions = self._predict_tflite(processed_image)
        
        # Post-process
        return self._format_predictions(predictions)
```

### Database Schema

#### crop_health_analysis Table

```sql
CREATE TABLE crop_health_analysis (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES auth.users(id),
    field_id UUID REFERENCES fields(id),
    image_url TEXT,
    diagnosis TEXT,
    confidence FLOAT,
    severity TEXT,
    treatment_recommendation TEXT,
    crop_type TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## Performance Metrics

### Model Performance

| Metric | Value |
|--------|-------|
| Validation Accuracy | ~90-95% |
| Inference Time | 100-200ms |
| Model Size (TFLite) | ~16MB |
| Input Resolution | 224Ã—224 |
| Classes Supported | 38 |
| Plants Supported | 14 |

### System Performance

| Metric | Value |
|--------|-------|
| API Response Time | <1 second |
| Image Upload Limit | 10MB |
| Concurrent Users | 100+ |
| Database Queries | <100ms |

---

## Future Enhancements

### Planned Features

1. **Crop Recommendation Integration**
   - Add `/recommend-crop` endpoint
   - Integrate Random Forest model
   - Display recommendations on Dashboard

2. **Yield Prediction**
   - Time-series forecasting
   - Historical data analysis
   - Weather integration

3. **Field Efficiency Optimization**
   - Resource usage analysis
   - Cost optimization
   - Irrigation scheduling

4. **Harvest Planning**
   - Optimal harvest date prediction
   - Market price integration
   - Labor scheduling

5. **Weather Integration**
   - Real-time weather data
   - Forecast-based recommendations
   - Climate risk assessment

6. **Mobile App**
   - Android/iOS apps
   - Offline support (TFLite on device)
   - GPS-based field mapping

7. **Drone Integration**
   - Aerial image analysis
   - Large-scale field monitoring
   - Automated disease detection

---

## References

### Research Papers

1. **Hughes, D. P., & Salathe, M.** (2016). An Open Access Repository of Images on Plant Health to Enable the Development of Mobile Disease Diagnostics. *ArXiv:1511.08060*

2. **Howard, A. G., et al.** (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. *ArXiv:1704.04861*

### Datasets

1. **PlantVillage Dataset** - 87,000 images, 38 disease classes
2. **Crop Recommendation Dataset** - Soil and weather parameters, 22 crops

### Technologies

1. **TensorFlow** - https://www.tensorflow.org/
2. **FastAPI** - https://fastapi.tiangolo.com/
3. **Supabase** - https://supabase.com/
4. **React** - https://react.dev/

---

## Conclusion

Smart Fasal combines cutting-edge machine learning with practical agricultural needs to create a comprehensive precision farming platform. The Crop Disease Detection model, based on MobileNet and trained on the PlantVillage dataset, achieves ~90% accuracy in identifying plant diseases from leaf images. With FastAPI backend, TFLite deployment, and Supabase integration, the system provides fast, accurate, and scalable disease detection for farmers.

Future integration of additional ML models (crop recommendation, yield prediction, etc.) will make Smart Fasal a complete AI-powered farming assistant.

---

**Version**: 1.0  
**Last Updated**: October 30, 2025  
**Author**: Smart Fasal Team


